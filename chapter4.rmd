# 4. Clustering and classification 

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```



```{r, echo=FALSE, warning=FALSE}
library(MASS)
library(tidyr)
library(corrplot)
library(magrittr)
library(dplyr)
library(reshape2)
library(ggplot2)
```

## Let's load the data and describe it


```{r}
data(Boston)
str(Boston)
```
Housing values in Suburbs of Boston gives info on houses and areas in Boston. See full description of the variables below: 

Description

The Boston data frame has 506 rows and 14 columns.
Usage

Boston

Format

This data frame contains the following columns:

crim

    per capita crime rate by town.
zn

    proportion of residential land zoned for lots over 25,000 sq.ft.
indus

    proportion of non-retail business acres per town.
chas

    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox

    nitrogen oxides concentration (parts per 10 million).
rm

    average number of rooms per dwelling.
age

    proportion of owner-occupied units built prior to 1940.
dis

    weighted mean of distances to five Boston employment centres.
rad

    index of accessibility to radial highways.
tax

    full-value property-tax rate per $10,000.
ptratio

    pupil-teacher ratio by town.
black

    1000(Bk−0.63)21000(Bk - 0.63)^21000(Bk−0.63)2 where BkBkBk is the proportion of blacks by town.
lstat

    lower status of the population (percent).
medv

    median value of owner-occupied homes in $1000s.

Source

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley. 

## Graphical overview

### First, I'll plot the distributions. Before that, I'll reshape the data from wide to long.

```{r}
library(reshape2)

# Melt the dataset
long_data <- melt(Boston)

# Plot the distributions
ggplot(long_data, aes(x = value)) +
  geom_histogram(binwidth = 1, fill = "pink", color = "black") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()+
  labs(title="Distirbution of different variables")

```
Looking at the distributions one can see that crim, zn, age dis and rad are skewed

## Next, I will plot the correlation matrix
```{r}

# correlation matrix
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle")

```

Looking at the correlation plot, one can see that crime rate is positively correlated with index of accessibility to radial highways and property tax rate. I think it makes sense that crime rate is higher in the area where highways are close but I think it is surprising that higher property tax is correlated with higher crime rate. I would have imagine that higher tax rates singnals higher income and therefore lower crime rate in the area.

### Standardizing data

```{r}
# Scale the Boston data set and save to boston_scaled
boston_scaled <- scale(Boston) %>% as.data.frame()

summary(boston_scaled)
round(var(boston_scaled),2)
```


```{r}
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins,labels = c("low","med_low","med_high","high"), include.lowest = TRUE)

# look at the table of the new factor crime


# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```


```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# boston_scaled is available

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```



```{r}
# Work with the exercise in this chunk, step-by-step. Fix the R code!
# data train is available

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```

```{r}

correct_classes <- test$crime
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted =lda.pred$class)

```



```{r}
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary (dist_eu)

```


```{r}

set.seed(123)

# determine the number of clusters
k_max <- 12

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)


```


